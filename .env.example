# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_key
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-reasoner

# Implementer LLM (e.g., Gemini or local Ollama)
IMPLEMENTER_API_KEY=your_api_key
IMPLEMENTER_BASE_URL=
IMPLEMENTER_MODEL=gemini-2.5-pro

# GitHub (optional, needed for private PRs)
GITHUB_TOKEN=your_github_token

# Workflow
MAX_LOC_PER_BATCH=2000
OUT_DIR=out
INCREMENTAL_COMMENTS=true
POST_SUMMARY_COMMENT=false
SKIP_DOC_REVIEWS=true
MAX_REVIEW_AGENTS=3
REVIEW_ONLY=false
DRY_RUN_COMMENTS=false

# Optional CI hooks
TEST_CMD=
COVERAGE_CMD=

# Performance Optimization (reduce CPU/memory usage)
# Set USE_DEEPSEEK_ONLY=true to skip Ollama entirely (recommended for speed)
USE_DEEPSEEK_ONLY=false

# Ollama Performance Settings (if using local models)
OLLAMA_NUM_THREADS=2          # Limit to 2 CPU threads (reduce from default)
OLLAMA_CONTEXT_SIZE=2048      # Smaller context = faster
OLLAMA_BATCH_SIZE=64          # Smaller batch = less memory


# Workflow Performance
PARALLEL_AGENTS=false         # Run agents in sequence (lower CPU)
MAX_PATCH_SIZE=500            # Max lines per review (reduce for speed)
SKIP_TEST_REVIEWS=true        # Don't review test files (faster)
ENABLE_CACHE=true             # Cache similar reviews
CACHE_TTL=3600                # Cache duration in seconds
